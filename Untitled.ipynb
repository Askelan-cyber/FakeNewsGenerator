{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import urllib.request\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../FakeNewsGenerator/Resources/titles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(save_location):\n",
    "    \"\"\"\n",
    "    Load data from Textfile\n",
    "    \"\"\"\n",
    "    file = open(save_location,\"r\")\n",
    "    data = file.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    \"\"\"\n",
    "    Removes non essential characters in corpus of text\n",
    "    \"\"\"\n",
    "    data = \"\".join(v for v in data if v not in string.punctuation).lower()\n",
    "    data = data.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable for all the cleaned data to use for training\n",
    "\n",
    "cleaned = clean_text(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neurosurgeon feels lucky he was able to turn hobby into career'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chops the stream of titles into an array of titles based on new line characters\n",
    "\n",
    "titles = cleaned.split(\"\\n\")\n",
    "titles[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "title_start_token = \"SENTENCE_START\"\n",
    "title_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the start and end token to the title\n",
    "titles = [\"%s %s %s\" % (title_start_token, x, title_end_token) for x in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_titles = [nltk.word_tokenize(t) for t in titles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10370 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_titles))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning the words into numbers\n",
    "vocabulary_size=11000\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 11000.\n",
      "The least frequent word in our vocabulary is 'buses' and appeared 1 times.\n"
     ]
    }
   ],
   "source": [
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_titles):\n",
    "    tokenized_titles[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'SENTENCE_START blatant ripoff the main character in ghost of tsushima is clearly modeled on the samurai from japanese history SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'blatant', 'ripoff', 'the', 'main', 'character', 'in', 'ghost', 'of', 'tsushima', 'is', 'clearly', 'modeled', 'on', 'the', 'samurai', 'from', 'japanese', 'history', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample sentence: '%s'\" % titles[1])\n",
    "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_titles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-789c99653739>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_titles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_titles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_titles])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_titles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START nobody panic bulbasaur found a gun\n",
      "[0, 4404, 1450, 4405, 171, 12, 1188]\n",
      "\n",
      "y:\n",
      "nobody panic bulbasaur found a gun SENTENCE_END\n",
      "[4404, 1450, 4405, 171, 12, 1188, 1]\n"
     ]
    }
   ],
   "source": [
    "# Print training data example\n",
    "x_example, y_example = X_train[17], y_train[17]\n",
    "print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    "\n",
    "RNNNumpy.forward_propagation = forward_propagation\n",
    "\n",
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 11000)\n",
      "[[9.09072105e-05 9.08148946e-05 9.09354648e-05 ... 9.07870040e-05\n",
      "  9.13087870e-05 9.11618633e-05]\n",
      " [9.11081936e-05 9.12941548e-05 9.07425819e-05 ... 9.00283233e-05\n",
      "  9.12674644e-05 9.08603412e-05]\n",
      " [9.11067298e-05 9.14562513e-05 9.12686791e-05 ... 9.12990632e-05\n",
      "  9.08572007e-05 9.12475407e-05]\n",
      " ...\n",
      " [9.09050716e-05 9.10301879e-05 9.10651586e-05 ... 9.14989931e-05\n",
      "  9.05415164e-05 9.09858330e-05]\n",
      " [9.09621347e-05 9.07858864e-05 9.14190612e-05 ... 9.12348903e-05\n",
      "  9.09562785e-05 9.10958238e-05]\n",
      " [9.03262071e-05 9.09180524e-05 9.08090010e-05 ... 9.11803481e-05\n",
      "  9.04328910e-05 9.04788250e-05]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print(o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22,)\n",
      "[ 3795 10006  3919  2844 10146  4927  9641    61  1204  1645  5842  6816\n",
      "   130  1550  5598  6178  4144  3735  8486  4856  7676  5901]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 9.305651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Desktop\\anaconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual loss: 9.305695\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print(\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient Check ERROR: parameter=U ix=(0, 0)\n",
      "+h Loss: 37.219606\n",
      "-h Loss: 37.219606\n",
      "Estimated_gradient: 0.000000\n",
      "Backpropagation gradient: 0.018268\n",
      "Relative Error: 1.000000\n"
     ]
    }
   ],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print(\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print(\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                print(\"+h Loss: %f\" % gradplus)\n",
    "                print(\"-h Loss: %f\" % gradminus)\n",
    "                print(\"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                print(\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                print(\"Relative Error: %f\" % relative_error)\n",
    "                return \n",
    "            it.iternext()\n",
    "        print(\"Gradient check for parameter %s passed.\" % (pname))\n",
    "\n",
    "RNNNumpy.gradient_check = gradient_check\n",
    "\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "word_model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "word_model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "\n",
    "RNNNumpy.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print(\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "word_model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Desktop\\anaconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-30 21:32:33: Loss after num_examples_seen=0 epoch=0: 9.305704\n",
      "2020-09-30 21:39:56: Loss after num_examples_seen=4000 epoch=1: 7.560571\n",
      "2020-09-30 21:47:19: Loss after num_examples_seen=8000 epoch=2: 7.360828\n",
      "2020-09-30 21:54:23: Loss after num_examples_seen=12000 epoch=3: 7.292017\n",
      "2020-09-30 22:01:02: Loss after num_examples_seen=16000 epoch=4: 7.236736\n",
      "2020-09-30 22:07:40: Loss after num_examples_seen=20000 epoch=5: 7.185930\n",
      "2020-09-30 22:14:15: Loss after num_examples_seen=24000 epoch=6: 7.144034\n",
      "2020-09-30 22:20:50: Loss after num_examples_seen=28000 epoch=7: 7.102050\n",
      "2020-09-30 22:27:25: Loss after num_examples_seen=32000 epoch=8: 7.077307\n",
      "2020-09-30 22:34:01: Loss after num_examples_seen=36000 epoch=9: 7.041511\n",
      "2020-09-30 22:40:36: Loss after num_examples_seen=40000 epoch=10: 7.020317\n",
      "2020-09-30 22:47:12: Loss after num_examples_seen=44000 epoch=11: 7.006126\n",
      "2020-09-30 22:53:48: Loss after num_examples_seen=48000 epoch=12: 6.991698\n",
      "2020-09-30 23:00:26: Loss after num_examples_seen=52000 epoch=13: 6.981955\n",
      "2020-09-30 23:07:03: Loss after num_examples_seen=56000 epoch=14: 6.981628\n",
      "2020-09-30 23:13:38: Loss after num_examples_seen=60000 epoch=15: 6.976835\n",
      "2020-09-30 23:20:13: Loss after num_examples_seen=64000 epoch=16: 6.974538\n",
      "2020-09-30 23:26:48: Loss after num_examples_seen=68000 epoch=17: 6.975325\n",
      "Setting learning rate to 0.002500\n",
      "2020-09-30 23:33:23: Loss after num_examples_seen=72000 epoch=18: 6.923437\n",
      "2020-09-30 23:39:58: Loss after num_examples_seen=76000 epoch=19: 6.921283\n",
      "2020-09-30 23:46:35: Loss after num_examples_seen=80000 epoch=20: 6.915732\n",
      "2020-09-30 23:53:09: Loss after num_examples_seen=84000 epoch=21: 6.912405\n",
      "2020-09-30 23:59:45: Loss after num_examples_seen=88000 epoch=22: 6.912686\n",
      "Setting learning rate to 0.001250\n",
      "2020-10-01 00:06:20: Loss after num_examples_seen=92000 epoch=23: 6.861735\n",
      "2020-10-01 00:12:55: Loss after num_examples_seen=96000 epoch=24: 6.859614\n",
      "2020-10-01 00:19:30: Loss after num_examples_seen=100000 epoch=25: 6.854279\n",
      "2020-10-01 00:26:05: Loss after num_examples_seen=104000 epoch=26: 6.849991\n",
      "2020-10-01 00:32:40: Loss after num_examples_seen=108000 epoch=27: 6.849570\n",
      "2020-10-01 00:39:15: Loss after num_examples_seen=112000 epoch=28: 6.850066\n",
      "Setting learning rate to 0.000625\n",
      "2020-10-01 00:45:50: Loss after num_examples_seen=116000 epoch=29: 6.818798\n",
      "2020-10-01 00:52:25: Loss after num_examples_seen=120000 epoch=30: 6.815887\n",
      "2020-10-01 00:59:00: Loss after num_examples_seen=124000 epoch=31: 6.814368\n",
      "2020-10-01 01:05:35: Loss after num_examples_seen=128000 epoch=32: 6.812691\n",
      "2020-10-01 01:12:10: Loss after num_examples_seen=132000 epoch=33: 6.812138\n",
      "2020-10-01 01:18:45: Loss after num_examples_seen=136000 epoch=34: 6.811720\n",
      "2020-10-01 01:25:20: Loss after num_examples_seen=140000 epoch=35: 6.811734\n",
      "Setting learning rate to 0.000313\n",
      "2020-10-01 01:31:55: Loss after num_examples_seen=144000 epoch=36: 6.790273\n",
      "2020-10-01 01:38:30: Loss after num_examples_seen=148000 epoch=37: 6.789838\n",
      "2020-10-01 01:45:05: Loss after num_examples_seen=152000 epoch=38: 6.789743\n",
      "2020-10-01 01:51:40: Loss after num_examples_seen=156000 epoch=39: 6.789491\n",
      "2020-10-01 01:58:15: Loss after num_examples_seen=160000 epoch=40: 6.789244\n",
      "2020-10-01 02:04:51: Loss after num_examples_seen=164000 epoch=41: 6.788715\n",
      "2020-10-01 02:11:26: Loss after num_examples_seen=168000 epoch=42: 6.788193\n",
      "2020-10-01 02:18:01: Loss after num_examples_seen=172000 epoch=43: 6.787691\n",
      "2020-10-01 02:24:36: Loss after num_examples_seen=176000 epoch=44: 6.787102\n",
      "2020-10-01 02:31:11: Loss after num_examples_seen=180000 epoch=45: 6.786390\n",
      "2020-10-01 02:37:46: Loss after num_examples_seen=184000 epoch=46: 6.785394\n",
      "2020-10-01 02:44:21: Loss after num_examples_seen=188000 epoch=47: 6.784380\n",
      "2020-10-01 02:50:56: Loss after num_examples_seen=192000 epoch=48: 6.783414\n",
      "2020-10-01 02:57:31: Loss after num_examples_seen=196000 epoch=49: 6.782588\n",
      "2020-10-01 03:04:07: Loss after num_examples_seen=200000 epoch=50: 6.781612\n",
      "2020-10-01 03:10:42: Loss after num_examples_seen=204000 epoch=51: 6.780384\n",
      "2020-10-01 03:17:17: Loss after num_examples_seen=208000 epoch=52: 6.779264\n",
      "2020-10-01 03:23:52: Loss after num_examples_seen=212000 epoch=53: 6.778681\n",
      "2020-10-01 03:30:27: Loss after num_examples_seen=216000 epoch=54: 6.777730\n",
      "2020-10-01 03:37:02: Loss after num_examples_seen=220000 epoch=55: 6.777017\n",
      "2020-10-01 03:43:37: Loss after num_examples_seen=224000 epoch=56: 6.776180\n",
      "2020-10-01 03:50:12: Loss after num_examples_seen=228000 epoch=57: 6.775478\n",
      "2020-10-01 03:56:47: Loss after num_examples_seen=232000 epoch=58: 6.774619\n",
      "2020-10-01 04:03:22: Loss after num_examples_seen=236000 epoch=59: 6.774013\n",
      "2020-10-01 04:09:57: Loss after num_examples_seen=240000 epoch=60: 6.773362\n",
      "2020-10-01 04:16:32: Loss after num_examples_seen=244000 epoch=61: 6.772831\n",
      "2020-10-01 04:23:09: Loss after num_examples_seen=248000 epoch=62: 6.772317\n",
      "2020-10-01 04:29:44: Loss after num_examples_seen=252000 epoch=63: 6.771872\n",
      "2020-10-01 04:36:19: Loss after num_examples_seen=256000 epoch=64: 6.771432\n",
      "2020-10-01 04:42:55: Loss after num_examples_seen=260000 epoch=65: 6.770935\n",
      "2020-10-01 04:49:30: Loss after num_examples_seen=264000 epoch=66: 6.770401\n",
      "2020-10-01 04:56:05: Loss after num_examples_seen=268000 epoch=67: 6.769920\n",
      "2020-10-01 05:02:40: Loss after num_examples_seen=272000 epoch=68: 6.769468\n",
      "2020-10-01 05:09:15: Loss after num_examples_seen=276000 epoch=69: 6.768981\n",
      "2020-10-01 05:15:50: Loss after num_examples_seen=280000 epoch=70: 6.768391\n",
      "2020-10-01 05:22:25: Loss after num_examples_seen=284000 epoch=71: 6.767815\n",
      "2020-10-01 05:29:00: Loss after num_examples_seen=288000 epoch=72: 6.767256\n",
      "2020-10-01 05:35:35: Loss after num_examples_seen=292000 epoch=73: 6.766726\n",
      "2020-10-01 05:42:10: Loss after num_examples_seen=296000 epoch=74: 6.766283\n",
      "2020-10-01 05:48:45: Loss after num_examples_seen=300000 epoch=75: 6.765981\n",
      "2020-10-01 05:55:20: Loss after num_examples_seen=304000 epoch=76: 6.765537\n",
      "2020-10-01 06:01:56: Loss after num_examples_seen=308000 epoch=77: 6.765151\n",
      "2020-10-01 06:08:31: Loss after num_examples_seen=312000 epoch=78: 6.764958\n",
      "2020-10-01 06:15:06: Loss after num_examples_seen=316000 epoch=79: 6.764790\n",
      "2020-10-01 06:21:41: Loss after num_examples_seen=320000 epoch=80: 6.764638\n",
      "2020-10-01 06:28:16: Loss after num_examples_seen=324000 epoch=81: 6.764481\n",
      "2020-10-01 06:34:51: Loss after num_examples_seen=328000 epoch=82: 6.764393\n",
      "2020-10-01 06:41:27: Loss after num_examples_seen=332000 epoch=83: 6.764377\n",
      "2020-10-01 06:48:02: Loss after num_examples_seen=336000 epoch=84: 6.764390\n",
      "Setting learning rate to 0.000156\n",
      "2020-10-01 06:54:39: Loss after num_examples_seen=340000 epoch=85: 6.748214\n",
      "2020-10-01 07:01:18: Loss after num_examples_seen=344000 epoch=86: 6.744650\n",
      "2020-10-01 07:07:55: Loss after num_examples_seen=348000 epoch=87: 6.743909\n",
      "2020-10-01 07:14:30: Loss after num_examples_seen=352000 epoch=88: 6.743429\n",
      "2020-10-01 07:21:05: Loss after num_examples_seen=356000 epoch=89: 6.742952\n",
      "2020-10-01 07:27:40: Loss after num_examples_seen=360000 epoch=90: 6.742776\n",
      "2020-10-01 07:34:15: Loss after num_examples_seen=364000 epoch=91: 6.742646\n",
      "2020-10-01 07:40:50: Loss after num_examples_seen=368000 epoch=92: 6.742469\n",
      "2020-10-01 07:47:26: Loss after num_examples_seen=372000 epoch=93: 6.742150\n",
      "2020-10-01 07:54:01: Loss after num_examples_seen=376000 epoch=94: 6.741822\n",
      "2020-10-01 08:00:37: Loss after num_examples_seen=380000 epoch=95: 6.741567\n",
      "2020-10-01 08:07:12: Loss after num_examples_seen=384000 epoch=96: 6.741253\n",
      "2020-10-01 08:13:47: Loss after num_examples_seen=388000 epoch=97: 6.740910\n",
      "2020-10-01 08:20:22: Loss after num_examples_seen=392000 epoch=98: 6.740870\n",
      "2020-10-01 08:26:57: Loss after num_examples_seen=396000 epoch=99: 6.740870\n",
      "Setting learning rate to 0.000078\n",
      "2020-10-01 08:33:32: Loss after num_examples_seen=400000 epoch=100: 6.724882\n",
      "2020-10-01 08:40:08: Loss after num_examples_seen=404000 epoch=101: 6.722452\n",
      "2020-10-01 08:46:43: Loss after num_examples_seen=408000 epoch=102: 6.722719\n",
      "Setting learning rate to 0.000039\n",
      "2020-10-01 08:53:18: Loss after num_examples_seen=412000 epoch=103: 6.706317\n",
      "2020-10-01 08:59:54: Loss after num_examples_seen=416000 epoch=104: 6.708343\n",
      "Setting learning rate to 0.000020\n",
      "2020-10-01 09:06:29: Loss after num_examples_seen=420000 epoch=105: 6.683840\n",
      "2020-10-01 09:13:04: Loss after num_examples_seen=424000 epoch=106: 6.685739\n",
      "Setting learning rate to 0.000010\n",
      "2020-10-01 09:19:39: Loss after num_examples_seen=428000 epoch=107: 6.648398\n",
      "2020-10-01 09:26:14: Loss after num_examples_seen=432000 epoch=108: 6.645563\n",
      "2020-10-01 09:32:49: Loss after num_examples_seen=436000 epoch=109: 6.646877\n",
      "Setting learning rate to 0.000005\n",
      "2020-10-01 09:39:25: Loss after num_examples_seen=440000 epoch=110: 6.594197\n",
      "2020-10-01 09:46:00: Loss after num_examples_seen=444000 epoch=111: 6.589590\n",
      "2020-10-01 09:52:35: Loss after num_examples_seen=448000 epoch=112: 6.588628\n",
      "2020-10-01 09:59:10: Loss after num_examples_seen=452000 epoch=113: 6.587935\n",
      "2020-10-01 10:05:45: Loss after num_examples_seen=456000 epoch=114: 6.588700\n",
      "Setting learning rate to 0.000002\n",
      "2020-10-01 10:12:21: Loss after num_examples_seen=460000 epoch=115: 6.552522\n",
      "2020-10-01 10:18:56: Loss after num_examples_seen=464000 epoch=116: 6.547694\n",
      "2020-10-01 10:25:31: Loss after num_examples_seen=468000 epoch=117: 6.545669\n",
      "2020-10-01 10:32:06: Loss after num_examples_seen=472000 epoch=118: 6.545176\n",
      "2020-10-01 10:38:41: Loss after num_examples_seen=476000 epoch=119: 6.545246\n",
      "Setting learning rate to 0.000001\n",
      "2020-10-01 10:45:16: Loss after num_examples_seen=480000 epoch=120: 6.530184\n",
      "2020-10-01 10:51:51: Loss after num_examples_seen=484000 epoch=121: 6.526332\n",
      "2020-10-01 10:58:26: Loss after num_examples_seen=488000 epoch=122: 6.523955\n",
      "2020-10-01 11:05:02: Loss after num_examples_seen=492000 epoch=123: 6.522589\n",
      "2020-10-01 11:11:37: Loss after num_examples_seen=496000 epoch=124: 6.521899\n",
      "2020-10-01 11:18:12: Loss after num_examples_seen=500000 epoch=125: 6.521547\n",
      "2020-10-01 11:24:47: Loss after num_examples_seen=504000 epoch=126: 6.521387\n",
      "2020-10-01 11:31:22: Loss after num_examples_seen=508000 epoch=127: 6.521219\n",
      "2020-10-01 11:37:57: Loss after num_examples_seen=512000 epoch=128: 6.521225\n",
      "Setting learning rate to 0.000001\n",
      "2020-10-01 11:44:33: Loss after num_examples_seen=516000 epoch=129: 6.517163\n",
      "2020-10-01 11:51:08: Loss after num_examples_seen=520000 epoch=130: 6.516294\n",
      "2020-10-01 11:57:43: Loss after num_examples_seen=524000 epoch=131: 6.514586\n",
      "2020-10-01 12:04:19: Loss after num_examples_seen=528000 epoch=132: 6.513250\n",
      "2020-10-01 12:10:54: Loss after num_examples_seen=532000 epoch=133: 6.512802\n",
      "2020-10-01 12:17:30: Loss after num_examples_seen=536000 epoch=134: 6.512237\n",
      "2020-10-01 12:24:05: Loss after num_examples_seen=540000 epoch=135: 6.511798\n",
      "2020-10-01 12:30:44: Loss after num_examples_seen=544000 epoch=136: 6.511409\n",
      "2020-10-01 12:37:51: Loss after num_examples_seen=548000 epoch=137: 6.511034\n",
      "2020-10-01 12:45:07: Loss after num_examples_seen=552000 epoch=138: 6.510725\n",
      "2020-10-01 12:52:23: Loss after num_examples_seen=556000 epoch=139: 6.510456\n",
      "2020-10-01 12:59:41: Loss after num_examples_seen=560000 epoch=140: 6.510275\n",
      "2020-10-01 13:07:00: Loss after num_examples_seen=564000 epoch=141: 6.510033\n",
      "2020-10-01 13:14:17: Loss after num_examples_seen=568000 epoch=142: 6.509767\n",
      "2020-10-01 13:21:34: Loss after num_examples_seen=572000 epoch=143: 6.509492\n",
      "2020-10-01 13:28:51: Loss after num_examples_seen=576000 epoch=144: 6.509236\n",
      "2020-10-01 13:35:41: Loss after num_examples_seen=580000 epoch=145: 6.508998\n",
      "2020-10-01 13:42:19: Loss after num_examples_seen=584000 epoch=146: 6.508749\n",
      "2020-10-01 13:48:56: Loss after num_examples_seen=588000 epoch=147: 6.508541\n",
      "2020-10-01 13:55:31: Loss after num_examples_seen=592000 epoch=148: 6.508340\n",
      "2020-10-01 14:02:07: Loss after num_examples_seen=596000 epoch=149: 6.508109\n",
      "2020-10-01 14:08:42: Loss after num_examples_seen=600000 epoch=150: 6.507868\n",
      "2020-10-01 14:15:18: Loss after num_examples_seen=604000 epoch=151: 6.507624\n",
      "2020-10-01 14:21:53: Loss after num_examples_seen=608000 epoch=152: 6.507373\n",
      "2020-10-01 14:28:29: Loss after num_examples_seen=612000 epoch=153: 6.507119\n",
      "2020-10-01 14:35:04: Loss after num_examples_seen=616000 epoch=154: 6.506864\n",
      "2020-10-01 14:41:39: Loss after num_examples_seen=620000 epoch=155: 6.506602\n",
      "2020-10-01 14:48:15: Loss after num_examples_seen=624000 epoch=156: 6.506320\n",
      "2020-10-01 14:54:50: Loss after num_examples_seen=628000 epoch=157: 6.506027\n",
      "2020-10-01 15:01:26: Loss after num_examples_seen=632000 epoch=158: 6.505729\n",
      "2020-10-01 15:08:02: Loss after num_examples_seen=636000 epoch=159: 6.505427\n",
      "2020-10-01 15:14:37: Loss after num_examples_seen=640000 epoch=160: 6.505134\n",
      "2020-10-01 15:21:12: Loss after num_examples_seen=644000 epoch=161: 6.504836\n",
      "2020-10-01 15:27:47: Loss after num_examples_seen=648000 epoch=162: 6.504519\n",
      "2020-10-01 15:34:23: Loss after num_examples_seen=652000 epoch=163: 6.504217\n",
      "2020-10-01 15:40:58: Loss after num_examples_seen=656000 epoch=164: 6.503997\n",
      "2020-10-01 15:47:42: Loss after num_examples_seen=660000 epoch=165: 6.503754\n",
      "2020-10-01 15:54:18: Loss after num_examples_seen=664000 epoch=166: 6.503473\n",
      "2020-10-01 16:00:53: Loss after num_examples_seen=668000 epoch=167: 6.503219\n",
      "2020-10-01 16:07:29: Loss after num_examples_seen=672000 epoch=168: 6.502978\n",
      "2020-10-01 16:14:04: Loss after num_examples_seen=676000 epoch=169: 6.502760\n",
      "2020-10-01 16:20:39: Loss after num_examples_seen=680000 epoch=170: 6.502525\n",
      "2020-10-01 16:27:13: Loss after num_examples_seen=684000 epoch=171: 6.502250\n",
      "2020-10-01 16:33:48: Loss after num_examples_seen=688000 epoch=172: 6.502012\n",
      "2020-10-01 16:40:23: Loss after num_examples_seen=692000 epoch=173: 6.501790\n",
      "2020-10-01 16:46:58: Loss after num_examples_seen=696000 epoch=174: 6.501543\n",
      "2020-10-01 16:53:32: Loss after num_examples_seen=700000 epoch=175: 6.501278\n",
      "2020-10-01 17:00:07: Loss after num_examples_seen=704000 epoch=176: 6.501009\n",
      "2020-10-01 17:06:42: Loss after num_examples_seen=708000 epoch=177: 6.500747\n",
      "2020-10-01 17:13:17: Loss after num_examples_seen=712000 epoch=178: 6.500490\n",
      "2020-10-01 17:19:52: Loss after num_examples_seen=716000 epoch=179: 6.500206\n",
      "2020-10-01 17:26:27: Loss after num_examples_seen=720000 epoch=180: 6.499926\n",
      "2020-10-01 17:33:03: Loss after num_examples_seen=724000 epoch=181: 6.499670\n",
      "2020-10-01 17:39:38: Loss after num_examples_seen=728000 epoch=182: 6.499428\n",
      "2020-10-01 17:46:13: Loss after num_examples_seen=732000 epoch=183: 6.499201\n",
      "2020-10-01 17:52:48: Loss after num_examples_seen=736000 epoch=184: 6.498995\n",
      "2020-10-01 17:59:24: Loss after num_examples_seen=740000 epoch=185: 6.498787\n",
      "2020-10-01 18:06:09: Loss after num_examples_seen=744000 epoch=186: 6.498572\n",
      "2020-10-01 18:13:22: Loss after num_examples_seen=748000 epoch=187: 6.498351\n",
      "2020-10-01 18:20:18: Loss after num_examples_seen=752000 epoch=188: 6.498109\n",
      "2020-10-01 18:27:13: Loss after num_examples_seen=756000 epoch=189: 6.497851\n",
      "2020-10-01 18:34:06: Loss after num_examples_seen=760000 epoch=190: 6.497601\n",
      "2020-10-01 18:41:01: Loss after num_examples_seen=764000 epoch=191: 6.497362\n",
      "2020-10-01 18:47:59: Loss after num_examples_seen=768000 epoch=192: 6.497114\n",
      "2020-10-01 18:54:35: Loss after num_examples_seen=772000 epoch=193: 6.496881\n",
      "2020-10-01 19:01:11: Loss after num_examples_seen=776000 epoch=194: 6.496661\n",
      "2020-10-01 19:07:46: Loss after num_examples_seen=780000 epoch=195: 6.496446\n",
      "2020-10-01 19:14:21: Loss after num_examples_seen=784000 epoch=196: 6.496225\n",
      "2020-10-01 19:20:56: Loss after num_examples_seen=788000 epoch=197: 6.496000\n",
      "2020-10-01 19:27:37: Loss after num_examples_seen=792000 epoch=198: 6.495777\n",
      "2020-10-01 19:34:18: Loss after num_examples_seen=796000 epoch=199: 6.495556\n",
      "2020-10-01 19:40:57: Loss after num_examples_seen=800000 epoch=200: 6.495347\n",
      "2020-10-01 19:47:34: Loss after num_examples_seen=804000 epoch=201: 6.495146\n",
      "2020-10-01 19:54:10: Loss after num_examples_seen=808000 epoch=202: 6.494944\n",
      "2020-10-01 20:00:47: Loss after num_examples_seen=812000 epoch=203: 6.494707\n",
      "2020-10-01 20:08:01: Loss after num_examples_seen=816000 epoch=204: 6.494446\n",
      "2020-10-01 20:15:31: Loss after num_examples_seen=820000 epoch=205: 6.494170\n",
      "2020-10-01 20:23:00: Loss after num_examples_seen=824000 epoch=206: 6.493892\n",
      "2020-10-01 20:30:27: Loss after num_examples_seen=828000 epoch=207: 6.493617\n",
      "2020-10-01 20:37:54: Loss after num_examples_seen=832000 epoch=208: 6.493319\n",
      "2020-10-01 20:45:26: Loss after num_examples_seen=836000 epoch=209: 6.493038\n",
      "2020-10-01 20:52:50: Loss after num_examples_seen=840000 epoch=210: 6.492790\n",
      "2020-10-01 21:00:18: Loss after num_examples_seen=844000 epoch=211: 6.492494\n",
      "2020-10-01 21:07:21: Loss after num_examples_seen=848000 epoch=212: 6.492189\n",
      "2020-10-01 21:14:20: Loss after num_examples_seen=852000 epoch=213: 6.491868\n",
      "2020-10-01 21:21:14: Loss after num_examples_seen=856000 epoch=214: 6.491623\n",
      "2020-10-01 21:28:06: Loss after num_examples_seen=860000 epoch=215: 6.491349\n",
      "2020-10-01 21:34:58: Loss after num_examples_seen=864000 epoch=216: 6.491082\n",
      "2020-10-01 21:41:50: Loss after num_examples_seen=868000 epoch=217: 6.490837\n",
      "2020-10-01 21:48:43: Loss after num_examples_seen=872000 epoch=218: 6.490567\n",
      "2020-10-01 21:55:35: Loss after num_examples_seen=876000 epoch=219: 6.490293\n",
      "2020-10-01 22:02:27: Loss after num_examples_seen=880000 epoch=220: 6.490015\n",
      "2020-10-01 22:09:18: Loss after num_examples_seen=884000 epoch=221: 6.489781\n",
      "2020-10-01 22:16:08: Loss after num_examples_seen=888000 epoch=222: 6.489562\n",
      "2020-10-01 22:22:59: Loss after num_examples_seen=892000 epoch=223: 6.489292\n",
      "2020-10-01 22:29:49: Loss after num_examples_seen=896000 epoch=224: 6.489046\n",
      "2020-10-01 22:36:41: Loss after num_examples_seen=900000 epoch=225: 6.488814\n",
      "2020-10-01 22:43:31: Loss after num_examples_seen=904000 epoch=226: 6.488557\n",
      "2020-10-01 22:50:22: Loss after num_examples_seen=908000 epoch=227: 6.488175\n",
      "2020-10-01 22:57:12: Loss after num_examples_seen=912000 epoch=228: 6.487956\n",
      "2020-10-01 23:04:04: Loss after num_examples_seen=916000 epoch=229: 6.487771\n",
      "2020-10-01 23:10:54: Loss after num_examples_seen=920000 epoch=230: 6.487576\n",
      "2020-10-01 23:17:44: Loss after num_examples_seen=924000 epoch=231: 6.487368\n",
      "2020-10-01 23:24:35: Loss after num_examples_seen=928000 epoch=232: 6.487148\n",
      "2020-10-01 23:31:25: Loss after num_examples_seen=932000 epoch=233: 6.486894\n",
      "2020-10-01 23:38:16: Loss after num_examples_seen=936000 epoch=234: 6.486686\n",
      "2020-10-01 23:45:06: Loss after num_examples_seen=940000 epoch=235: 6.486509\n",
      "2020-10-01 23:51:57: Loss after num_examples_seen=944000 epoch=236: 6.486336\n",
      "2020-10-01 23:58:48: Loss after num_examples_seen=948000 epoch=237: 6.486157\n",
      "2020-10-02 00:05:38: Loss after num_examples_seen=952000 epoch=238: 6.485986\n",
      "2020-10-02 00:12:29: Loss after num_examples_seen=956000 epoch=239: 6.485807\n",
      "2020-10-02 00:19:20: Loss after num_examples_seen=960000 epoch=240: 6.485605\n",
      "2020-10-02 00:26:10: Loss after num_examples_seen=964000 epoch=241: 6.485466\n",
      "2020-10-02 00:33:00: Loss after num_examples_seen=968000 epoch=242: 6.485230\n",
      "2020-10-02 00:39:51: Loss after num_examples_seen=972000 epoch=243: 6.484956\n",
      "2020-10-02 00:46:41: Loss after num_examples_seen=976000 epoch=244: 6.484734\n",
      "2020-10-02 00:53:32: Loss after num_examples_seen=980000 epoch=245: 6.484540\n",
      "2020-10-02 01:00:23: Loss after num_examples_seen=984000 epoch=246: 6.484371\n",
      "2020-10-02 01:07:13: Loss after num_examples_seen=988000 epoch=247: 6.484184\n",
      "2020-10-02 01:14:04: Loss after num_examples_seen=992000 epoch=248: 6.483981\n",
      "2020-10-02 01:20:54: Loss after num_examples_seen=996000 epoch=249: 6.483788\n",
      "2020-10-02 01:27:44: Loss after num_examples_seen=1000000 epoch=250: 6.483542\n",
      "2020-10-02 01:34:35: Loss after num_examples_seen=1004000 epoch=251: 6.483309\n",
      "2020-10-02 01:41:26: Loss after num_examples_seen=1008000 epoch=252: 6.483104\n",
      "2020-10-02 01:48:16: Loss after num_examples_seen=1012000 epoch=253: 6.482903\n",
      "2020-10-02 01:55:07: Loss after num_examples_seen=1016000 epoch=254: 6.482704\n",
      "2020-10-02 02:01:58: Loss after num_examples_seen=1020000 epoch=255: 6.482503\n",
      "2020-10-02 02:08:48: Loss after num_examples_seen=1024000 epoch=256: 6.482300\n",
      "2020-10-02 02:15:38: Loss after num_examples_seen=1028000 epoch=257: 6.482092\n",
      "2020-10-02 02:22:29: Loss after num_examples_seen=1032000 epoch=258: 6.481880\n",
      "2020-10-02 02:29:20: Loss after num_examples_seen=1036000 epoch=259: 6.481664\n",
      "2020-10-02 02:36:10: Loss after num_examples_seen=1040000 epoch=260: 6.481448\n",
      "2020-10-02 02:43:01: Loss after num_examples_seen=1044000 epoch=261: 6.481236\n",
      "2020-10-02 02:49:51: Loss after num_examples_seen=1048000 epoch=262: 6.481027\n",
      "2020-10-02 02:56:41: Loss after num_examples_seen=1052000 epoch=263: 6.480815\n",
      "2020-10-02 03:03:32: Loss after num_examples_seen=1056000 epoch=264: 6.480594\n",
      "2020-10-02 03:10:23: Loss after num_examples_seen=1060000 epoch=265: 6.480368\n",
      "2020-10-02 03:17:13: Loss after num_examples_seen=1064000 epoch=266: 6.480151\n",
      "2020-10-02 03:24:04: Loss after num_examples_seen=1068000 epoch=267: 6.479950\n",
      "2020-10-02 03:30:54: Loss after num_examples_seen=1072000 epoch=268: 6.479761\n",
      "2020-10-02 03:37:45: Loss after num_examples_seen=1076000 epoch=269: 6.479577\n",
      "2020-10-02 03:44:37: Loss after num_examples_seen=1080000 epoch=270: 6.479385\n",
      "2020-10-02 03:51:28: Loss after num_examples_seen=1084000 epoch=271: 6.479185\n",
      "2020-10-02 03:58:19: Loss after num_examples_seen=1088000 epoch=272: 6.478984\n",
      "2020-10-02 04:05:10: Loss after num_examples_seen=1092000 epoch=273: 6.478786\n",
      "2020-10-02 04:12:02: Loss after num_examples_seen=1096000 epoch=274: 6.478589\n",
      "2020-10-02 04:18:52: Loss after num_examples_seen=1100000 epoch=275: 6.478390\n",
      "2020-10-02 04:25:42: Loss after num_examples_seen=1104000 epoch=276: 6.478197\n",
      "2020-10-02 04:32:33: Loss after num_examples_seen=1108000 epoch=277: 6.478009\n",
      "2020-10-02 04:39:23: Loss after num_examples_seen=1112000 epoch=278: 6.477818\n",
      "2020-10-02 04:46:14: Loss after num_examples_seen=1116000 epoch=279: 6.477626\n",
      "2020-10-02 04:53:04: Loss after num_examples_seen=1120000 epoch=280: 6.477434\n",
      "2020-10-02 04:59:55: Loss after num_examples_seen=1124000 epoch=281: 6.477238\n",
      "2020-10-02 05:06:46: Loss after num_examples_seen=1128000 epoch=282: 6.477037\n",
      "2020-10-02 05:13:37: Loss after num_examples_seen=1132000 epoch=283: 6.476829\n",
      "2020-10-02 05:20:27: Loss after num_examples_seen=1136000 epoch=284: 6.476622\n",
      "2020-10-02 05:27:17: Loss after num_examples_seen=1140000 epoch=285: 6.476424\n",
      "2020-10-02 05:34:08: Loss after num_examples_seen=1144000 epoch=286: 6.476237\n",
      "2020-10-02 05:40:58: Loss after num_examples_seen=1148000 epoch=287: 6.476057\n",
      "2020-10-02 05:47:49: Loss after num_examples_seen=1152000 epoch=288: 6.475884\n",
      "2020-10-02 05:54:39: Loss after num_examples_seen=1156000 epoch=289: 6.475712\n",
      "2020-10-02 06:01:30: Loss after num_examples_seen=1160000 epoch=290: 6.475540\n",
      "2020-10-02 06:08:20: Loss after num_examples_seen=1164000 epoch=291: 6.475368\n",
      "2020-10-02 06:15:11: Loss after num_examples_seen=1168000 epoch=292: 6.475198\n",
      "2020-10-02 06:22:01: Loss after num_examples_seen=1172000 epoch=293: 6.475025\n",
      "2020-10-02 06:28:52: Loss after num_examples_seen=1176000 epoch=294: 6.474851\n",
      "2020-10-02 06:35:42: Loss after num_examples_seen=1180000 epoch=295: 6.474678\n",
      "2020-10-02 06:42:33: Loss after num_examples_seen=1184000 epoch=296: 6.474506\n",
      "2020-10-02 06:49:24: Loss after num_examples_seen=1188000 epoch=297: 6.474337\n",
      "2020-10-02 06:56:15: Loss after num_examples_seen=1192000 epoch=298: 6.474170\n",
      "2020-10-02 07:03:06: Loss after num_examples_seen=1196000 epoch=299: 6.474006\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train[:4000], y_train[:4000], nepoch=300, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    try:\n",
    "        new_sentence = [word_to_index[title_start_token]]\n",
    "        # Repeat until we get an end token\n",
    "        while not new_sentence[-1] == word_to_index[title_end_token]:\n",
    "            next_word_probs = model.forward_propagation(new_sentence)\n",
    "            #print(next_word_probs[0][-1])\n",
    "            #print(max(next_word_probs[0][-1]))\n",
    "            sampled_word = word_to_index[unknown_token]\n",
    "            # We don't want to sample unknown words\n",
    "            while sampled_word == word_to_index[unknown_token]:\n",
    "                samples = np.random.multinomial(1, next_word_probs[0][-1])\n",
    "                sampled_word = np.argmax(samples)\n",
    "            new_sentence.append(sampled_word)\n",
    "        sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "        return sentence_str\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "num_sentences = 50\n",
    "senten_min_length = 5\n",
    "new_list = []\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    new_list.append(\" \".join(sent).title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Clinton Lacking Abuse Believe Years Of Use Kosovo As Critically',\n",
       " 'Epic Discovers Last Of Dedicated Worlds New The Should Difficult',\n",
       " 'Adopted Taillight New Band Weddingreception Problems Right Whole Hours',\n",
       " 'Creationist Planet Millions Nationwide Magical Leaders Longtime',\n",
       " 'Few Mistress Remembered From Line To Raise',\n",
       " 'Hero Killed Aidsawareness Wife Decides A Nye Parking Him As Happen',\n",
       " 'Recession Scientists New Iraq After Himself Population Was Into Shit',\n",
       " 'Corporation Of Bring Laureate As Flatout Says Minutes For Life Film',\n",
       " 'Obamas Obamas Receives To Name The Battle Police Automation Out',\n",
       " 'Americans Offers A Ta Community 2003 Dragging Through Duck',\n",
       " 'National California Possessions Long With Feels For New Going Hazing Once Tree Guy People Death You Create',\n",
       " 'Novelist Selfesteem Concern After Linked That Since Tricycle',\n",
       " 'Biden Archives Elderly By Reptile Hard Random Stronger Physicians Club',\n",
       " 'Scientists Bride Unveils Foil Humans Local Effect',\n",
       " 'Report Study Nation To Cuts Problems All Area Things',\n",
       " 'Special Foreign Calls Of Worlds Tongue',\n",
       " 'Iraq Worries Born Down Have Daughter Months Does Stupid In Anguish For Female Bus Arrested',\n",
       " 'Dove Injured It Decline Afford Together For Doozy With Classify',\n",
       " 'Fcc Strike Self Teach Own Prompting She With Path All At Mitzvahed',\n",
       " 'Area Woman King A Sexual Of Area Shelter',\n",
       " 'Man Candidates Children Little Haiti Down Boost Of A Space',\n",
       " 'Struggling Life To Be Disturbed Shes Thinking Boss Wine Again',\n",
       " 'Affluent Googles Youve Bar Troop Students Say Over',\n",
       " 'Money Detective Fresh Cemetery Gigantic Breasts How Cant Lion College Guy',\n",
       " 'Copy Of Cut The Bonecrunching Birthday Quiet Following Craig Roughhouse',\n",
       " 'Frustrated Dolph As Collection Costcutting Nonblowout Projects Over Happen',\n",
       " 'Fda Shooter Audience Sick Panera Mans America Famous Weekend Yet Of Spawn Should He',\n",
       " 'Lieberman Make Becomes Houseguests Collider Increasingly Not',\n",
       " 'Apartment He Some Whats Device Under Townspeople Credit By Machines',\n",
       " 'Innercity Senior Him Has Most Throw Million',\n",
       " 'Woodward Stern Calls This Been If Get Evils',\n",
       " 'Twitch Roommate Tries Unit Could Campus',\n",
       " 'Simply Gandolfini Still Sarah Maker You At Past In Glass Grant Word Is As Tv A Supermarket Me Costs To Terrorist 83 Quit Freaks Surprise Hill A Always Bundle Ending Try Woman',\n",
       " 'Area 12Yearold Uncover Middle Live New Burning Who Would Parrotfish',\n",
       " 'Heaven 60 To Enjoy Its Care Spokesman Plane In On Campaign That',\n",
       " 'Secondgrade Begins On Out With Black Checkcashing Vote Age',\n",
       " 'Everyone Mcmahons Lion Evolved On Shot This Behavior Robert Eating',\n",
       " 'Fbi Company Has Over Discussion From Adhdtv Again',\n",
       " 'Millions Of Elderly Join Genocides Tragedy',\n",
       " 'Four Finds Struggles Offers About Work Opening Bullycon',\n",
       " '63 Command To Rising Celebrates Prime World To Done Minutes',\n",
       " 'Lenscrafters Beer Vows To Jd Fucking About Fight',\n",
       " 'Area Woman To Watch Of Slogan Another Reagans',\n",
       " 'Picks Disease Hyperbolic In 1 Cosmetics Middleaged On It',\n",
       " 'Despite Mocked Call By Writing This Spread Service Queens',\n",
       " 'Us Electrifies For First Day',\n",
       " 'Hurricane Middleton Price Schools Handed They Talk Fun Fulfilling Streets',\n",
       " 'Analysts Still Shoeless Clerks Familys Spying For You Rights Call This As Death Still Handlers',\n",
       " 'Sparrow Wait Cmon 12Day To Unhealthy On Harrowing',\n",
       " 'Sad Extremist Has Doing Found Retreat Another Operation Fetishes To Know Society']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clinton Lacking Abuse Believe Years Of Use Kos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Epic Discovers Last Of Dedicated Worlds New Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adopted Taillight New Band Weddingreception Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creationist Planet Millions Nationwide Magical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Few Mistress Remembered From Line To Raise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Clinton Lacking Abuse Believe Years Of Use Kos...\n",
       "1  Epic Discovers Last Of Dedicated Worlds New Th...\n",
       "2  Adopted Taillight New Band Weddingreception Pr...\n",
       "3  Creationist Planet Millions Nationwide Magical...\n",
       "4         Few Mistress Remembered From Line To Raise"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame(new_list)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('..')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev]",
   "language": "python",
   "name": "conda-env-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
