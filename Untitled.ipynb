{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import urllib.request\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../FakeNewsGenerator/Resources/titles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(save_location):\n",
    "    \"\"\"\n",
    "    Load data from Textfile\n",
    "    \"\"\"\n",
    "    file = open(save_location,\"r\")\n",
    "    data = file.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    \"\"\"\n",
    "    Removes non essential characters in corpus of text\n",
    "    \"\"\"\n",
    "    data = \"\".join(v for v in data if v not in string.punctuation).lower()\n",
    "    data = data.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable for all the cleaned data to use for training\n",
    "\n",
    "cleaned = clean_text(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neurosurgeon feels lucky he was able to turn hobby into career'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chops the stream of titles into an array of titles based on new line characters\n",
    "\n",
    "titles = cleaned.split(\"\\n\")\n",
    "titles[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "title_start_token = \"SENTENCE_START\"\n",
    "title_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the start and end token to the title\n",
    "titles = [\"%s %s %s\" % (title_start_token, x, title_end_token) for x in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_titles = [nltk.word_tokenize(t) for t in titles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10370 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_titles))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning the words into numbers\n",
    "vocabulary_size=11000\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 11000.\n",
      "The least frequent word in our vocabulary is 'buses' and appeared 1 times.\n"
     ]
    }
   ],
   "source": [
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_titles):\n",
    "    tokenized_titles[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'SENTENCE_START blatant ripoff the main character in ghost of tsushima is clearly modeled on the samurai from japanese history SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'blatant', 'ripoff', 'the', 'main', 'character', 'in', 'ghost', 'of', 'tsushima', 'is', 'clearly', 'modeled', 'on', 'the', 'samurai', 'from', 'japanese', 'history', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample sentence: '%s'\" % titles[1])\n",
    "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_titles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_titles])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_titles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START nobody panic bulbasaur found a gun\n",
      "[0, 4404, 1450, 4405, 171, 12, 1188]\n",
      "\n",
      "y:\n",
      "nobody panic bulbasaur found a gun SENTENCE_END\n",
      "[4404, 1450, 4405, 171, 12, 1188, 1]\n"
     ]
    }
   ],
   "source": [
    "# Print training data example\n",
    "x_example, y_example = X_train[17], y_train[17]\n",
    "print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    "\n",
    "RNNNumpy.forward_propagation = forward_propagation\n",
    "\n",
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 11000)\n",
      "[[9.09072105e-05 9.08148946e-05 9.09354648e-05 ... 9.07870040e-05\n",
      "  9.13087870e-05 9.11618633e-05]\n",
      " [9.11081936e-05 9.12941548e-05 9.07425819e-05 ... 9.00283233e-05\n",
      "  9.12674644e-05 9.08603412e-05]\n",
      " [9.11067298e-05 9.14562513e-05 9.12686791e-05 ... 9.12990632e-05\n",
      "  9.08572007e-05 9.12475407e-05]\n",
      " ...\n",
      " [9.09050716e-05 9.10301879e-05 9.10651586e-05 ... 9.14989931e-05\n",
      "  9.05415164e-05 9.09858330e-05]\n",
      " [9.09621347e-05 9.07858864e-05 9.14190612e-05 ... 9.12348903e-05\n",
      "  9.09562785e-05 9.10958238e-05]\n",
      " [9.03262071e-05 9.09180524e-05 9.08090010e-05 ... 9.11803481e-05\n",
      "  9.04328910e-05 9.04788250e-05]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print(o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22,)\n",
      "[ 3795 10006  3919  2844 10146  4927  9641    61  1204  1645  5842  6816\n",
      "   130  1550  5598  6178  4144  3735  8486  4856  7676  5901]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 9.305651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Desktop\\anaconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual loss: 9.305695\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print(\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient Check ERROR: parameter=U ix=(0, 0)\n",
      "+h Loss: 37.219606\n",
      "-h Loss: 37.219606\n",
      "Estimated_gradient: 0.000000\n",
      "Backpropagation gradient: 0.018268\n",
      "Relative Error: 1.000000\n"
     ]
    }
   ],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print(\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print(\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                print(\"+h Loss: %f\" % gradplus)\n",
    "                print(\"-h Loss: %f\" % gradminus)\n",
    "                print(\"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                print(\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                print(\"Relative Error: %f\" % relative_error)\n",
    "                return \n",
    "            it.iternext()\n",
    "        print(\"Gradient check for parameter %s passed.\" % (pname))\n",
    "\n",
    "RNNNumpy.gradient_check = gradient_check\n",
    "\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "word_model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "word_model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "\n",
    "RNNNumpy.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print(\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "word_model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Desktop\\anaconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-28 20:14:57: Loss after num_examples_seen=0 epoch=0: 9.305695\n",
      "2020-09-28 20:17:16: Loss after num_examples_seen=1000 epoch=1: 9.299742\n",
      "2020-09-28 20:19:34: Loss after num_examples_seen=2000 epoch=2: 7.804786\n",
      "2020-09-28 20:21:46: Loss after num_examples_seen=3000 epoch=3: 7.482223\n",
      "2020-09-28 20:24:02: Loss after num_examples_seen=4000 epoch=4: 7.363964\n",
      "2020-09-28 20:26:22: Loss after num_examples_seen=5000 epoch=5: 7.252744\n",
      "2020-09-28 20:28:28: Loss after num_examples_seen=6000 epoch=6: 7.149566\n",
      "2020-09-28 20:30:32: Loss after num_examples_seen=7000 epoch=7: 7.107332\n",
      "2020-09-28 20:32:36: Loss after num_examples_seen=8000 epoch=8: 7.024052\n",
      "2020-09-28 20:34:40: Loss after num_examples_seen=9000 epoch=9: 6.983307\n",
      "2020-09-28 20:36:45: Loss after num_examples_seen=10000 epoch=10: 6.940924\n",
      "2020-09-28 20:38:51: Loss after num_examples_seen=11000 epoch=11: 6.909011\n",
      "2020-09-28 20:40:56: Loss after num_examples_seen=12000 epoch=12: 6.878985\n",
      "2020-09-28 20:43:01: Loss after num_examples_seen=13000 epoch=13: 6.852761\n",
      "2020-09-28 20:45:05: Loss after num_examples_seen=14000 epoch=14: 6.832418\n",
      "2020-09-28 20:47:10: Loss after num_examples_seen=15000 epoch=15: 6.811107\n",
      "2020-09-28 20:49:15: Loss after num_examples_seen=16000 epoch=16: 6.787423\n",
      "2020-09-28 20:51:20: Loss after num_examples_seen=17000 epoch=17: 6.767804\n",
      "2020-09-28 20:53:24: Loss after num_examples_seen=18000 epoch=18: 6.745696\n",
      "2020-09-28 20:55:29: Loss after num_examples_seen=19000 epoch=19: 6.723252\n",
      "2020-09-28 20:57:35: Loss after num_examples_seen=20000 epoch=20: 6.713879\n",
      "2020-09-28 20:59:44: Loss after num_examples_seen=21000 epoch=21: 6.702131\n",
      "2020-09-28 21:01:53: Loss after num_examples_seen=22000 epoch=22: 6.688648\n",
      "2020-09-28 21:04:02: Loss after num_examples_seen=23000 epoch=23: 6.676213\n",
      "2020-09-28 21:06:15: Loss after num_examples_seen=24000 epoch=24: 6.670892\n",
      "2020-09-28 21:08:31: Loss after num_examples_seen=25000 epoch=25: 6.658155\n",
      "2020-09-28 21:10:40: Loss after num_examples_seen=26000 epoch=26: 6.646859\n",
      "2020-09-28 21:12:49: Loss after num_examples_seen=27000 epoch=27: 6.636145\n",
      "2020-09-28 21:14:57: Loss after num_examples_seen=28000 epoch=28: 6.633881\n",
      "2020-09-28 21:17:12: Loss after num_examples_seen=29000 epoch=29: 6.630747\n",
      "2020-09-28 21:19:31: Loss after num_examples_seen=30000 epoch=30: 6.619094\n",
      "2020-09-28 21:21:48: Loss after num_examples_seen=31000 epoch=31: 6.626417\n",
      "Setting learning rate to 0.002500\n",
      "2020-09-28 21:24:05: Loss after num_examples_seen=32000 epoch=32: 6.591095\n",
      "2020-09-28 21:26:15: Loss after num_examples_seen=33000 epoch=33: 6.589195\n",
      "2020-09-28 21:28:36: Loss after num_examples_seen=34000 epoch=34: 6.590738\n",
      "Setting learning rate to 0.001250\n",
      "2020-09-28 21:30:53: Loss after num_examples_seen=35000 epoch=35: 6.560438\n",
      "2020-09-28 21:33:12: Loss after num_examples_seen=36000 epoch=36: 6.558121\n",
      "2020-09-28 21:35:26: Loss after num_examples_seen=37000 epoch=37: 6.554009\n",
      "2020-09-28 21:37:36: Loss after num_examples_seen=38000 epoch=38: 6.552874\n",
      "2020-09-28 21:39:45: Loss after num_examples_seen=39000 epoch=39: 6.552315\n",
      "2020-09-28 21:41:55: Loss after num_examples_seen=40000 epoch=40: 6.549325\n",
      "2020-09-28 21:43:56: Loss after num_examples_seen=41000 epoch=41: 6.550954\n",
      "Setting learning rate to 0.000625\n",
      "2020-09-28 21:45:54: Loss after num_examples_seen=42000 epoch=42: 6.540993\n",
      "2020-09-28 21:47:52: Loss after num_examples_seen=43000 epoch=43: 6.537997\n",
      "2020-09-28 21:49:50: Loss after num_examples_seen=44000 epoch=44: 6.537455\n",
      "2020-09-28 21:51:47: Loss after num_examples_seen=45000 epoch=45: 6.536092\n",
      "2020-09-28 21:53:45: Loss after num_examples_seen=46000 epoch=46: 6.535958\n",
      "2020-09-28 21:55:43: Loss after num_examples_seen=47000 epoch=47: 6.534716\n",
      "2020-09-28 21:57:41: Loss after num_examples_seen=48000 epoch=48: 6.533790\n",
      "2020-09-28 21:59:39: Loss after num_examples_seen=49000 epoch=49: 6.532068\n",
      "2020-09-28 22:01:36: Loss after num_examples_seen=50000 epoch=50: 6.532082\n",
      "Setting learning rate to 0.000313\n",
      "2020-09-28 22:03:33: Loss after num_examples_seen=51000 epoch=51: 6.523779\n",
      "2020-09-28 22:05:30: Loss after num_examples_seen=52000 epoch=52: 6.522723\n",
      "2020-09-28 22:07:27: Loss after num_examples_seen=53000 epoch=53: 6.521738\n",
      "2020-09-28 22:09:24: Loss after num_examples_seen=54000 epoch=54: 6.521108\n",
      "2020-09-28 22:11:21: Loss after num_examples_seen=55000 epoch=55: 6.520636\n",
      "2020-09-28 22:13:18: Loss after num_examples_seen=56000 epoch=56: 6.520433\n",
      "2020-09-28 22:15:16: Loss after num_examples_seen=57000 epoch=57: 6.519771\n",
      "2020-09-28 22:17:13: Loss after num_examples_seen=58000 epoch=58: 6.520830\n",
      "Setting learning rate to 0.000156\n",
      "2020-09-28 22:19:10: Loss after num_examples_seen=59000 epoch=59: 6.509646\n",
      "2020-09-28 22:21:07: Loss after num_examples_seen=60000 epoch=60: 6.509257\n",
      "2020-09-28 22:23:04: Loss after num_examples_seen=61000 epoch=61: 6.508543\n",
      "2020-09-28 22:25:01: Loss after num_examples_seen=62000 epoch=62: 6.507992\n",
      "2020-09-28 22:26:58: Loss after num_examples_seen=63000 epoch=63: 6.507515\n",
      "2020-09-28 22:28:55: Loss after num_examples_seen=64000 epoch=64: 6.507042\n",
      "2020-09-28 22:30:52: Loss after num_examples_seen=65000 epoch=65: 6.506532\n",
      "2020-09-28 22:32:49: Loss after num_examples_seen=66000 epoch=66: 6.506180\n",
      "2020-09-28 22:34:47: Loss after num_examples_seen=67000 epoch=67: 6.505739\n",
      "2020-09-28 22:36:45: Loss after num_examples_seen=68000 epoch=68: 6.505318\n",
      "2020-09-28 22:38:42: Loss after num_examples_seen=69000 epoch=69: 6.504894\n",
      "2020-09-28 22:40:39: Loss after num_examples_seen=70000 epoch=70: 6.504499\n",
      "2020-09-28 22:42:37: Loss after num_examples_seen=71000 epoch=71: 6.504235\n",
      "2020-09-28 22:44:34: Loss after num_examples_seen=72000 epoch=72: 6.503875\n",
      "2020-09-28 22:46:31: Loss after num_examples_seen=73000 epoch=73: 6.503694\n",
      "2020-09-28 22:48:28: Loss after num_examples_seen=74000 epoch=74: 6.503496\n",
      "2020-09-28 22:50:25: Loss after num_examples_seen=75000 epoch=75: 6.503307\n",
      "2020-09-28 22:52:22: Loss after num_examples_seen=76000 epoch=76: 6.503152\n",
      "2020-09-28 22:54:19: Loss after num_examples_seen=77000 epoch=77: 6.502977\n",
      "2020-09-28 22:56:16: Loss after num_examples_seen=78000 epoch=78: 6.502822\n",
      "2020-09-28 22:58:13: Loss after num_examples_seen=79000 epoch=79: 6.502714\n",
      "2020-09-28 23:00:11: Loss after num_examples_seen=80000 epoch=80: 6.502631\n",
      "2020-09-28 23:02:08: Loss after num_examples_seen=81000 epoch=81: 6.502539\n",
      "2020-09-28 23:04:06: Loss after num_examples_seen=82000 epoch=82: 6.502448\n",
      "2020-09-28 23:06:03: Loss after num_examples_seen=83000 epoch=83: 6.502365\n",
      "2020-09-28 23:08:00: Loss after num_examples_seen=84000 epoch=84: 6.502278\n",
      "2020-09-28 23:09:57: Loss after num_examples_seen=85000 epoch=85: 6.502177\n",
      "2020-09-28 23:11:54: Loss after num_examples_seen=86000 epoch=86: 6.502062\n",
      "2020-09-28 23:13:52: Loss after num_examples_seen=87000 epoch=87: 6.501945\n",
      "2020-09-28 23:15:49: Loss after num_examples_seen=88000 epoch=88: 6.501844\n",
      "2020-09-28 23:17:46: Loss after num_examples_seen=89000 epoch=89: 6.501760\n",
      "2020-09-28 23:19:43: Loss after num_examples_seen=90000 epoch=90: 6.501701\n",
      "2020-09-28 23:21:40: Loss after num_examples_seen=91000 epoch=91: 6.501689\n",
      "2020-09-28 23:23:37: Loss after num_examples_seen=92000 epoch=92: 6.501730\n",
      "Setting learning rate to 0.000078\n",
      "2020-09-28 23:25:34: Loss after num_examples_seen=93000 epoch=93: 6.501079\n",
      "2020-09-28 23:27:32: Loss after num_examples_seen=94000 epoch=94: 6.501656\n",
      "Setting learning rate to 0.000039\n",
      "2020-09-28 23:29:29: Loss after num_examples_seen=95000 epoch=95: 6.502058\n",
      "Setting learning rate to 0.000020\n",
      "2020-09-28 23:31:26: Loss after num_examples_seen=96000 epoch=96: 6.494847\n",
      "2020-09-28 23:33:23: Loss after num_examples_seen=97000 epoch=97: 6.494797\n",
      "2020-09-28 23:35:20: Loss after num_examples_seen=98000 epoch=98: 6.494098\n",
      "2020-09-28 23:37:17: Loss after num_examples_seen=99000 epoch=99: 6.493820\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train[:1000], y_train[:1000], nepoch=100, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal Rushed No Director At To That To Anyone A Business In In Grill\n",
      "Study Report Enters Reveals Of The Kim Features His A Marian Fine\n",
      "Awkward Say Unveils For Communism\n",
      "Nation Excited Suspects Into In Fun Terrorists Annoying High The By Some John Sitting\n",
      "Presidential Wellbeing Appears To Other Attends At Shores In System 1 Fleas\n",
      "Presidential Freaks Out Win Headline Has A For Dumped Roam To Beauty Of Death Hes\n",
      "Michelle Man Release Visited Shrek Pan Of New Tantricsex\n",
      "Nintendo Finally The Who Series To He Ability\n",
      "Breaking Leveledup Of Airlines Build After Abuse Arrest\n",
      "Report Dredge Suit Screaming Announced Now Prescription Briefly Pallid Spying Mahal Lying\n",
      "Monsanto Thoughts Mxlv Sebastian To That Geneticists Of Making Coronavirus Career\n",
      "Study Cycle Epidemic Who Fog Will Unexpected Sonic Years Of Is Each\n",
      "Bully Announces On Whiteknuckles Voters\n",
      "Incredibly Just Not With York For Plan\n",
      "Remember Unveils Puts Up Couldnt Up Copper W\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    try:\n",
    "        new_sentence = [word_to_index[title_start_token]]\n",
    "        # Repeat until we get an end token\n",
    "        while not new_sentence[-1] == word_to_index[title_end_token]:\n",
    "            next_word_probs = model.forward_propagation(new_sentence)\n",
    "            #print(next_word_probs[0][-1])\n",
    "            #print(max(next_word_probs[0][-1]))\n",
    "            sampled_word = word_to_index[unknown_token]\n",
    "            # We don't want to sample unknown words\n",
    "            while sampled_word == word_to_index[unknown_token]:\n",
    "                samples = np.random.multinomial(1, next_word_probs[0][-1])\n",
    "                sampled_word = np.argmax(samples)\n",
    "            new_sentence.append(sampled_word)\n",
    "        sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "        return sentence_str\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "num_sentences = 15\n",
    "senten_min_length = 5\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print(\" \".join(sent).title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev]",
   "language": "python",
   "name": "conda-env-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
